{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/enron-email-dataset/emails.csv\n",
      "/kaggle/input/final-project-datasetpkl/final_project_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "original = \"/kaggle/input/final-project-datasetpkl/final_project_dataset.pkl\"\n",
    "destination = \"modified_final_project_dataset.pkl\"\n",
    "\n",
    "content = ''\n",
    "outsize = 0\n",
    "with open(original, 'rb') as infile:\n",
    "    content = infile.read()\n",
    "with open(destination, 'wb') as output:\n",
    "    for line in content.splitlines():\n",
    "        outsize += len(line) + 1\n",
    "        output.write(line + str.encode('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open(\"modified_final_project_dataset.pkl\", 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "['salary', 'to_messages', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'email_address', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'from_poi_to_this_person', 'exercised_stock_options', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
      "Number of features:21\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "temp = 0\n",
    "for key in data_dict:\n",
    "    if temp == 0:\n",
    "        for feature in data_dict[key]:\n",
    "            all_features.append(feature)\n",
    "        temp = 1\n",
    "print (\"Features:\" + \"\\n\"+ str(all_features))\n",
    "print (\"Number of features:\" +str(len(all_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**removing some data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salary': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'loan_advances': 'NaN',\n",
       " 'bonus': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'total_stock_value': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'other': 362096,\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'poi': False,\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'restricted_stock': 'NaN',\n",
       " 'director_fees': 'NaN'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salary': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'total_payments': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'bonus': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'total_stock_value': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'poi': False,\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'restricted_stock': 'NaN',\n",
       " 'director_fees': 'NaN'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop('LOCKHART EUGENE E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salary': 26704229,\n",
       " 'to_messages': 'NaN',\n",
       " 'deferral_payments': 32083396,\n",
       " 'total_payments': 309886585,\n",
       " 'loan_advances': 83925000,\n",
       " 'bonus': 97343619,\n",
       " 'email_address': 'NaN',\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'deferred_income': -27992891,\n",
       " 'total_stock_value': 434509511,\n",
       " 'expenses': 5235198,\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'from_messages': 'NaN',\n",
       " 'other': 42667589,\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'poi': False,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'restricted_stock': 130322299,\n",
       " 'director_fees': 1398517}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop(\"TOTAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Fraction(_messages, poi_messages):\n",
    "\n",
    "    if _messages != 'NaN'or poi_messages != 'NaN':\n",
    "        fraction = float(poi_messages) / float(_messages)\n",
    "    else:\n",
    "        fraction = 0\n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "    \n",
    "    from_poi_to_this_person = data[\"from_poi_to_this_person\"]\n",
    "    to_messages = data[\"to_messages\"]\n",
    "    fraction_from_poi = make_Fraction( to_messages, from_poi_to_this_person)\n",
    "    \n",
    "    data[\"fraction_from_poi\"] = fraction_from_poi\n",
    "    \n",
    "    from_this_person_to_poi = data[\"from_this_person_to_poi\"]\n",
    "    from_messages = data[\"from_messages\"]\n",
    "    fraction_to_poi = make_Fraction( from_messages, from_this_person_to_poi)\n",
    "    \n",
    "    data[\"fraction_to_poi\"] = fraction_to_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = []\n",
    "temp = 0\n",
    "for key in data_dict:\n",
    "    if temp == 0:\n",
    "        for feature in data_dict[key]:\n",
    "            new_features.append(feature)\n",
    "        temp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary',\n",
       " 'to_messages',\n",
       " 'deferral_payments',\n",
       " 'total_payments',\n",
       " 'loan_advances',\n",
       " 'bonus',\n",
       " 'email_address',\n",
       " 'restricted_stock_deferred',\n",
       " 'deferred_income',\n",
       " 'total_stock_value',\n",
       " 'expenses',\n",
       " 'from_poi_to_this_person',\n",
       " 'exercised_stock_options',\n",
       " 'from_messages',\n",
       " 'other',\n",
       " 'from_this_person_to_poi',\n",
       " 'poi',\n",
       " 'long_term_incentive',\n",
       " 'shared_receipt_with_poi',\n",
       " 'restricted_stock',\n",
       " 'director_fees',\n",
       " 'fraction_from_poi',\n",
       " 'fraction_to_poi']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_remove = [\"poi\", \"email_address\", \"from_messages\", \"to_messages\", \"from_poi_to_this_person\", \"from_this_person_to_poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees', 'fraction_from_poi', 'fraction_to_poi']\n"
     ]
    }
   ],
   "source": [
    "modified_feature = [\"poi\"]\n",
    "for feature in new_features:\n",
    "    if feature not in features_remove:\n",
    "       modified_feature.append(feature)\n",
    "print (\"features_list = {}\".format(modified_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modified_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print (\"error: key \", feature, \" not present\")\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_dict\n",
    "data = featureFormat(dataset, modified_feature, sort_keys = True)\n",
    "Y, X= targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "cv = StratifiedShuffleSplit(n_splits=1000, random_state=42)\n",
    "for train, test in cv.split(X, Y):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    Y_train, Y_test = Y[train], Y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 15 128 15\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test),len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectKBest(chi2, k=5) \n",
    "select.fit(X_train_minmax, Y_train)\n",
    "X_train_minmax_skb = select.transform(X_train_minmax)\n",
    "X_test_minmax_skb = select.transform(X_test_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.79853834, 0.07301844, 0.52588964, 0.17142857, 2.62543546,\n",
       "       0.00768846, 0.15128769, 3.90756737, 1.35844284, 3.62891357,\n",
       "       0.00516472, 0.63930146, 1.58438395, 1.56354953, 1.36026087,\n",
       "       0.6283748 , 2.3993314 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing modules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"K Nearest Neighbors\", \"RBF SVM\", \"Linear SVM\", \"Decision Tree\", \"Naive Bayes\", \n",
    "         \"AdaBoost\", \"Random Forest\", \"Logistic Regression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [KNeighborsClassifier(), SVC(kernel='rbf', random_state=42), \\\n",
    "                SVC(kernel=\"linear\", random_state=42), DecisionTreeClassifier(random_state=42), \\\n",
    "                GaussianNB(), AdaBoostClassifier(random_state=42), RandomForestClassifier(random_state=42), \\\n",
    "                LogisticRegression(random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "confusion_matrices = []\n",
    "precisionscores = []\n",
    "recallscores = []\n",
    "accuracy = []\n",
    "table={}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train_minmax_skb, Y_train)\n",
    "    pred = clf.predict(X_test_minmax_skb)\n",
    "    conf_mat = confusion_matrix(Y_test, pred)\n",
    "    confusion_matrices.append(conf_mat)\n",
    "    precisionscore = round(precision_score(Y_test, pred), 3)\n",
    "    precisionscores.append(precisionscore)\n",
    "    recallscore = round(recall_score(Y_test, pred), 3)\n",
    "    recallscores.append(recallscore)\n",
    "    accuracy.append(accuracy_score(Y_test, pred))\n",
    "    \n",
    "    _scores = {}\n",
    "    _scores[\"Accuracy score\"] = accuracy_score(Y_test, pred)\n",
    "    _scores[\"Confusion matrix\"] = conf_mat\n",
    "    _scores[\"Precision score\"] =precisionscore\n",
    "    _scores[\"Recall score\"] = recallscore\n",
    "    table[name] = _scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Scaling: SelectKBest\n",
      "precisionscores ---- [1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0]\n",
      "Accuracy score----[0.9333333333333333, 0.8666666666666667, 0.8666666666666667, 0.8666666666666667, 0.8666666666666667, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333]\n",
      "Recall score----[0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "Confusion matrix----[array([[13,  0],\n",
      "       [ 1,  1]]), array([[13,  0],\n",
      "       [ 2,  0]]), array([[13,  0],\n",
      "       [ 2,  0]]), array([[12,  1],\n",
      "       [ 1,  1]]), array([[12,  1],\n",
      "       [ 1,  1]]), array([[13,  0],\n",
      "       [ 1,  1]]), array([[13,  0],\n",
      "       [ 1,  1]]), array([[13,  0],\n",
      "       [ 1,  1]])]\n"
     ]
    }
   ],
   "source": [
    "print (\"Feature Scaling: SelectKBest\")\n",
    "print(( \"precisionscores ---- \" +  str(precisionscores) +\"\\n\" + \"Accuracy score----\" +str(accuracy) +\"\\n\" +\"Recall score----\" + str(recallscores)+\"\\n\"+ \"Confusion matrix----\"+ str(confusion_matrices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data in a tabular fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K Nearest Neighbors</th>\n",
       "      <th>RBF SVM</th>\n",
       "      <th>Linear SVM</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy score</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Confusion matrix</th>\n",
       "      <td>[[13, 0], [1, 1]]</td>\n",
       "      <td>[[13, 0], [2, 0]]</td>\n",
       "      <td>[[13, 0], [2, 0]]</td>\n",
       "      <td>[[12, 1], [1, 1]]</td>\n",
       "      <td>[[12, 1], [1, 1]]</td>\n",
       "      <td>[[13, 0], [1, 1]]</td>\n",
       "      <td>[[13, 0], [1, 1]]</td>\n",
       "      <td>[[13, 0], [1, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision score</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall score</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 K Nearest Neighbors            RBF SVM         Linear SVM  \\\n",
       "Accuracy score              0.933333           0.866667           0.866667   \n",
       "Confusion matrix   [[13, 0], [1, 1]]  [[13, 0], [2, 0]]  [[13, 0], [2, 0]]   \n",
       "Precision score                    1                  0                  0   \n",
       "Recall score                     0.5                  0                  0   \n",
       "\n",
       "                      Decision Tree        Naive Bayes           AdaBoost  \\\n",
       "Accuracy score             0.866667           0.866667           0.933333   \n",
       "Confusion matrix  [[12, 1], [1, 1]]  [[12, 1], [1, 1]]  [[13, 0], [1, 1]]   \n",
       "Precision score                 0.5                0.5                  1   \n",
       "Recall score                    0.5                0.5                0.5   \n",
       "\n",
       "                      Random Forest Logistic Regression  \n",
       "Accuracy score             0.933333            0.933333  \n",
       "Confusion matrix  [[13, 0], [1, 1]]   [[13, 0], [1, 1]]  \n",
       "Precision score                   1                   1  \n",
       "Recall score                    0.5                 0.5  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY SCORE = 0.933333\n",
    "\n",
    "K Nearest Neighbors\n",
    "\n",
    "AdaBoost\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking AdaBoost as classifier for my_classifier.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=42)\n",
    "model.fit(X_train_minmax_skb, Y_train)\n",
    "print(accuracy_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_dict, open(\"mydataset.pkl\", \"wb\") )\n",
    "pickle.dump(model, open(\"myclassifier.pkl\", \"wb\") )\n",
    "pickle.dump(modified_feature, open(\" myfeaturelist.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
