{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading config and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-05 09:16:41--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 172.217.204.128, 74.125.31.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘uncased_L-12_H-768_A-12.zip.2’\n",
      "\n",
      "uncased_L-12_H-768_ 100%[===================>] 388.84M   141MB/s    in 2.7s    \n",
      "\n",
      "2020-09-05 09:16:44 (141 MB/s) - ‘uncased_L-12_H-768_A-12.zip.2’ saved [407727028/407727028]\n",
      "\n",
      "bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\n",
      "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "!ls 'uncased_L-12_H-768_A-12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some of the important scripts from the bert repo which you can find here(https://github.com/google-research/bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_uuid": "3e0ac6bb63d1487866640ebe8e73f78b3a96c25a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-05 09:16:50--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37922 (37K) [text/plain]\n",
      "Saving to: ‘modeling.py.2’\n",
      "\n",
      "modeling.py.2       100%[===================>]  37.03K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-09-05 09:16:50 (1.20 MB/s) - ‘modeling.py.2’ saved [37922/37922]\n",
      "\n",
      "--2020-09-05 09:16:51--  https://raw.githubusercontent.com/google-research/bert/master/extract_features.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13898 (14K) [text/plain]\n",
      "Saving to: ‘extract_features.py.2’\n",
      "\n",
      "extract_features.py 100%[===================>]  13.57K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-09-05 09:16:51 (555 KB/s) - ‘extract_features.py.2’ saved [13898/13898]\n",
      "\n",
      "--2020-09-05 09:16:52--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12257 (12K) [text/plain]\n",
      "Saving to: ‘tokenization.py.2’\n",
      "\n",
      "tokenization.py.2   100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-09-05 09:16:52 (24.4 MB/s) - ‘tokenization.py.2’ saved [12257/12257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "cfccbec6c87185a0db428e3ce8ecb93aa9c4547e"
   },
   "outputs": [],
   "source": [
    "import modeling\n",
    "import extract_features\n",
    "import tokenization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally downloading the data from the git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64c3f40f620c50434a4645a76fe5ad1c9d34ec74"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n",
    "!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f525be88f8fa80c8a8b680c0946e111cd7f653fa"
   },
   "source": [
    "Next, we feed BERT the data from these three files. For each line, we want to obtain contextual embeddings for the 3 target words (A, B, Pronoun). Here are some helper functions to keep track of the offsets of the target words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_uuid": "1a655a90d41802605da6f27c605313eac4af4cc2"
   },
   "outputs": [],
   "source": [
    "def compute_offset_no_spaces(text, offset):\n",
    "\tcount = 0\n",
    "\tfor pos in range(offset):\n",
    "\t\tif text[pos] != \" \": count +=1\n",
    "\treturn count\n",
    "\n",
    "def count_chars_no_special(text):\n",
    "\tcount = 0\n",
    "\tspecial_char_list = [\"#\"]\n",
    "\tfor pos in range(len(text)):\n",
    "\t\tif text[pos] not in special_char_list: count +=1\n",
    "\treturn count\n",
    "\n",
    "def count_length_no_special(text):\n",
    "\tcount = 0\n",
    "\tspecial_char_list = [\"#\", \" \"]\n",
    "\tfor pos in range(len(text)):\n",
    "\t\tif text[pos] not in special_char_list: count +=1\n",
    "\treturn count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_bert(data):\n",
    "# \t'''\n",
    "# \tRuns a forward propagation of BERT on input text, extracting contextual word embeddings\n",
    "# \tInput: data, a pandas DataFrame containing the information in one of the GAP files\n",
    "\n",
    "# \tOutput: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n",
    "# \tcolumns: \"emb_A\": the embedding for word A\n",
    "# \t         \"emb_B\": the embedding for word B\n",
    "# \t         \"emb_P\": the embedding for the pronoun\n",
    "# \t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "# \t'''\n",
    "#     # From the current file, take the text only, and write it in a file which will be passed to BERT\n",
    "# \ttext = data[\"Text\"]\n",
    "# \ttext.to_csv(\"input.txt\", index = False, header = False)\n",
    "\n",
    "#     # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n",
    "#     # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n",
    "# \tos.system(\"python3 extract_features.py \\\n",
    "# \t  --input_file=input.txt \\\n",
    "# \t  --output_file=output.jsonl \\\n",
    "# \t  --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "# \t  --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "# \t  --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "# \t  --layers=-1 \\\n",
    "# \t  --max_seq_length=256 \\\n",
    "# \t  --batch_size=8\")\n",
    "\n",
    "# \tbert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "\n",
    "# \tos.system(\"rm output.jsonl\")\n",
    "# \tos.system(\"rm input.txt\")\n",
    "\n",
    "# \tindex = data.index\n",
    "# \tcolumns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
    "# \temb = pd.DataFrame(index = index, columns = columns)\n",
    "# \temb.index.name = \"ID\"\n",
    "\n",
    "# \tfor i in range(len(data)): # For each line in the data file\n",
    "# \t\t# get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n",
    "# \t\tP = data.loc[i,\"Pronoun\"].lower()\n",
    "# \t\tA = data.loc[i,\"A\"].lower()\n",
    "# \t\tB = data.loc[i,\"B\"].lower()\n",
    "\n",
    "# \t\t# For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n",
    "# \t\tP_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n",
    "# \t\tA_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n",
    "# \t\tB_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n",
    "# \t\t# Figure out the length of A, B, not counting spaces or special characters\n",
    "# \t\tA_length = count_length_no_special(A)\n",
    "# \t\tB_length = count_length_no_special(B)\n",
    "\n",
    "# \t\t# Initialize embeddings with zeros\n",
    "# \t\temb_A = np.zeros(768)\n",
    "# \t\temb_B = np.zeros(768)\n",
    "# \t\temb_P = np.zeros(768)\n",
    "\n",
    "# \t\t# Initialize counts\n",
    "# \t\tcount_chars = 0\n",
    "# \t\tcnt_A, cnt_B, cnt_P = 0, 0, 0\n",
    "\n",
    "# \t\tfeatures = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n",
    "# \t\tfor j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n",
    "# \t\t\ttoken = features.loc[j,\"token\"]\n",
    "\n",
    "# \t\t\t# See if the character count until the current token matches the offset of any of the 3 target words\n",
    "# \t\t\tif count_chars  == P_offset: \n",
    "# \t\t\t\t# print(token)\n",
    "# \t\t\t\temb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "# \t\t\t\tcnt_P += 1\n",
    "# \t\t\tif count_chars in range(A_offset, A_offset + A_length): \n",
    "# \t\t\t\t# print(token)\n",
    "# \t\t\t\temb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "# \t\t\t\tcnt_A +=1\n",
    "# \t\t\tif count_chars in range(B_offset, B_offset + B_length): \n",
    "# \t\t\t\t# print(token)\n",
    "# \t\t\t\temb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "# \t\t\t\tcnt_B +=1\t\t\t\t\t\t\t\t\n",
    "# \t\t\t# Update the character count\n",
    "# \t\t\tcount_chars += count_length_no_special(token)\n",
    "# \t\t# Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n",
    "# \t\temb_A /= cnt_A\n",
    "# \t\temb_B /= cnt_B\n",
    "\n",
    "# \t\t# Work out the label of the current piece of text\n",
    "# \t\tlabel = \"Neither\"\n",
    "# \t\tif (data.loc[i,\"A-coref\"] == True):\n",
    "# \t\t\tlabel = \"A\"\n",
    "# \t\tif (data.loc[i,\"B-coref\"] == True):\n",
    "# \t\t\tlabel = \"B\"\n",
    "\n",
    "# \t\t# Put everything together in emb\n",
    "# \t\temb.iloc[i] = [emb_A, emb_B, emb_P, label]\n",
    "\n",
    "# \treturn emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6552a7a9786187610be695fcf25766594ca41685"
   },
   "source": [
    "The following method takes the data from a file, passes it through BERT to obtain contextual embeddings for the target words, then returns these embeddings in the emb DataFrame. Below, we will use it 3 times, once for each of the files gap-test, gap-development, gap-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_uuid": "9d8437cb4f7c1737e0c915d7d16cc3d004612416"
   },
   "outputs": [],
   "source": [
    "def run_bert(data):\n",
    "\t'''\n",
    "\tRuns a forward propagation of BERT on input text, extracting contextual word embeddings\n",
    "\tInput: data, a pandas DataFrame containing the information in one of the GAP files\n",
    "\n",
    "\tOutput: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n",
    "\tcolumns: \"emb_A\": the embedding for word A\n",
    "\t         \"emb_B\": the embedding for word B\n",
    "\t         \"emb_P\": the embedding for the pronoun\n",
    "\t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\t'''\n",
    "    # From the current file, take the text only, and write it in a file which will be passed to BERT\n",
    "\ttext = data[\"Text\"]\n",
    "\ttext.to_csv(\"input.txt\", index = False, header = False)\n",
    "\n",
    "    # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n",
    "    # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n",
    "\tos.system(\"python3 extract_features.py \\\n",
    "\t  --input_file=input.txt \\\n",
    "\t  --output_file=output.jsonl \\\n",
    "\t  --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "\t  --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "\t  --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "\t  --layers=-1 \\\n",
    "\t  --max_seq_length=256 \\\n",
    "\t  --batch_size=8\")\n",
    "\n",
    "\tbert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "\n",
    "\tos.system(\"rm output.jsonl\")\n",
    "\tos.system(\"rm input.txt\")\n",
    "\n",
    "\tindex = data.index\n",
    "\tcolumns = [\"emb_A\", \"emb_B\", \"emb_P\"]\n",
    "\temb = pd.DataFrame(index = index, columns = columns)\n",
    "\temb.index.name = \"ID\"\n",
    "\n",
    "\tfor i in range(len(data)): # For each line in the data file\n",
    "\t\t# get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n",
    "\t\tP = data.loc[i,\"Pronoun\"].lower()\n",
    "\t\tA = data.loc[i,\"A\"].lower()\n",
    "\t\tB = data.loc[i,\"B\"].lower()\n",
    "\n",
    "\t\t# For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n",
    "\t\tP_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n",
    "\t\tA_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n",
    "\t\tB_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n",
    "\t\t# Figure out the length of A, B, not counting spaces or special characters\n",
    "\t\tA_length = count_length_no_special(A)\n",
    "\t\tB_length = count_length_no_special(B)\n",
    "\n",
    "\t\t# Initialize embeddings with zeros\n",
    "\t\temb_A = np.zeros(768)\n",
    "\t\temb_B = np.zeros(768)\n",
    "\t\temb_P = np.zeros(768)\n",
    "\n",
    "\t\t# Initialize counts\n",
    "\t\tcount_chars = 0\n",
    "\t\tcnt_A, cnt_B, cnt_P = 0, 0, 0\n",
    "\n",
    "\t\tfeatures = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n",
    "\t\tfor j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n",
    "\t\t\ttoken = features.loc[j,\"token\"]\n",
    "\n",
    "\t\t\t# See if the character count until the current token matches the offset of any of the 3 target words\n",
    "\t\t\tif count_chars  == P_offset: \n",
    "\t\t\t\t# print(token)\n",
    "\t\t\t\temb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "\t\t\t\tcnt_P += 1\n",
    "\t\t\tif count_chars in range(A_offset, A_offset + A_length): \n",
    "\t\t\t\t# print(token)\n",
    "\t\t\t\temb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "\t\t\t\tcnt_A +=1\n",
    "\t\t\tif count_chars in range(B_offset, B_offset + B_length): \n",
    "\t\t\t\t# print(token)\n",
    "\t\t\t\temb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "\t\t\t\tcnt_B +=1\t\t\t\t\t\t\t\t\n",
    "\t\t\t# Update the character count\n",
    "\t\t\tcount_chars += count_length_no_special(token)\n",
    "\t\t# Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n",
    "\t\temb_A /= cnt_A\n",
    "\t\temb_B /= cnt_B\n",
    "\n",
    "# \t\t# Work out the label of the current piece of text\n",
    "# \t\tlabel = \"Neither\"\n",
    "# \t\tif (data.loc[i,\"A-coref\"] == True):\n",
    "# \t\t\tlabel = \"A\"\n",
    "# \t\tif (data.loc[i,\"B-coref\"] == True):\n",
    "# \t\t\tlabel = \"B\"\n",
    "\n",
    "\t\t# Put everything together in emb\n",
    "\t\temb.iloc[i] = [emb_A, emb_B, emb_P]\n",
    "\n",
    "\treturn emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98ca59d6fc859477aff649ea2ca8a35f65201e9d"
   },
   "source": [
    "Read the three GAP files, pass them through BERT, and write the contextual embeddings in json files. Unfortunately, I wasn't able to silence TensorFlow, so it's giving a lot of information and warnings when I run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8298947fa33285e722bdaae2df41bca5dd795732"
   },
   "outputs": [],
   "source": [
    "print(\"Started at \", time.ctime())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"gap-validation.tsv\", sep = '\\t')\n",
    "validation_emb = run_bert(validation_data)\n",
    "\n",
    "development_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\n",
    "development_emb = run_bert(development_data)\n",
    "\n",
    "print(\"Finished at \", time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(embedding_df):\n",
    "    \n",
    "    pronoun_embs, a_embs, b_embs = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(embedding_df))):\n",
    "        \n",
    "        pronoun_embs.append(embedding_df.loc[i, \"emb_P\"])\n",
    "        a_embs.append(embedding_df.loc[i, \"emb_A\"])\n",
    "        b_embs.append(embedding_df.loc[i, \"emb_B\"])\n",
    "\n",
    "#         label_map = {'A': 0, 'B': 1, 'Neither': 2}\n",
    "#         labels.append(label_map[embedding_df.loc[i, \"label\"]])\n",
    "\n",
    "    \n",
    "    a_embs = np.asarray(a_embs).astype('float')\n",
    "    b_embs = np.asarray(b_embs).astype('float') \n",
    "    pronoun_embs = np.asarray(pronoun_embs).astype('float')\n",
    "    \n",
    "    return np.concatenate([a_embs, b_embs, pronoun_embs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def featurize(embedding_df):\n",
    "    \n",
    "#     pronoun_embs, a_embs, b_embs, labels = [], [], [], []\n",
    "    \n",
    "#     for i in tqdm(range(len(embedding_df))):\n",
    "        \n",
    "#         pronoun_embs.append(embedding_df.loc[i, \"emb_P\"])\n",
    "#         a_embs.append(embedding_df.loc[i, \"emb_A\"])\n",
    "#         b_embs.append(embedding_df.loc[i, \"emb_B\"])\n",
    "\n",
    "#         label_map = {'A': 0, 'B': 1, 'Neither': 2}\n",
    "#         labels.append(label_map[embedding_df.loc[i, \"label\"]])\n",
    "\n",
    "    \n",
    "#     a_embs = np.asarray(a_embs).astype('float')\n",
    "#     b_embs = np.asarray(b_embs).astype('float') \n",
    "#     pronoun_embs = np.asarray(pronoun_embs).astype('float')\n",
    "    \n",
    "#     return np.concatenate([a_embs, b_embs, pronoun_embs], axis=1), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = featurize(pd.concat([validation_emb, development_emb]).sort_index().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = my_imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=0.0075, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
    "          n_jobs=4, penalty='l2', random_state=42, solver='lbfgs',\n",
    "          tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(logit, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../input/gendered-pronoun-resolution/test_stage_2.tsv\",sep = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('./finalized_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12359"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/gendered-pronoun-resolution/sample_submission_stage_2.csv\", index_col = \"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12359"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.235900e+04</td>\n",
       "      <td>1.235900e+04</td>\n",
       "      <td>1.235900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.324494e-14</td>\n",
       "      <td>4.324494e-14</td>\n",
       "      <td>4.324494e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "      <td>3.333300e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A             B       NEITHER\n",
       "count  1.235900e+04  1.235900e+04  1.235900e+04\n",
       "mean   3.333300e-01  3.333300e-01  3.333300e-01\n",
       "std    4.324494e-14  4.324494e-14  4.324494e-14\n",
       "min    3.333300e-01  3.333300e-01  3.333300e-01\n",
       "25%    3.333300e-01  3.333300e-01  3.333300e-01\n",
       "50%    3.333300e-01  3.333300e-01  3.333300e-01\n",
       "75%    3.333300e-01  3.333300e-01  3.333300e-01\n",
       "max    3.333300e-01  3.333300e-01  3.333300e-01"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19301.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 17992.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 20422.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19898.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16286.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16359.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18154.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 20098.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19762.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18617.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 18266.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19275.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 20157.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19575.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16819.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17947.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 19783.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 18643.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 20133.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 18430.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17324.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17001.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19760.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 14644.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 14864.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 20536.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16428.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19844.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17528.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19623.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18044.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18835.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18406.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18007.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17084.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17285.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19356.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16187.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19449.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19339.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 15990.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17818.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 12672.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18932.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16829.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 15904.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 200/200 [00:00<00:00, 18292.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 17375.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19122.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16771.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19824.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16564.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18658.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 16732.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19580.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18400.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19722.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 20441.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19441.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 20947.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19642.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [00:00<00:00, 15731.61it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 159 is out of bounds for axis 0 with size 159",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-2306017a75c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogit_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_test_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_test_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NEITHER\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlogit_test_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 159 is out of bounds for axis 0 with size 159"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0,12360, 200):\n",
    "    test_emb = run_bert(test_data[i:i+200].reset_index())\n",
    "    X_test = featurize(test_emb.sort_index().reset_index())\n",
    "    X_test = my_imputer.fit_transform(X_test)\n",
    "    logit_test_pred = loaded_model.predict_proba(X_test)\n",
    "    for j in range(0, 200):\n",
    "        submission.iloc[count+j][\"A\"] = logit_test_pred[j, 0]\n",
    "        submission.iloc[count+j][\"B\"] = logit_test_pred[j, 1]\n",
    "        submission.iloc[count+j][\"NEITHER\"]= logit_test_pred[j, 2]\n",
    "    count +=200\n",
    "    print(count)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12359"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ffe42e03c8033e9389c762f815075aa3</th>\n",
       "      <td>0.833172</td>\n",
       "      <td>0.163307</td>\n",
       "      <td>0.003521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffea29920b2ac0bd1fa25f70ace4a6db</th>\n",
       "      <td>0.056278</td>\n",
       "      <td>0.940258</td>\n",
       "      <td>0.003464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffec23d7150d0823fff87c3962c1ea52</th>\n",
       "      <td>0.858616</td>\n",
       "      <td>0.104715</td>\n",
       "      <td>0.036669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff8142c3dba443641dc233c049e2bf2</th>\n",
       "      <td>0.602934</td>\n",
       "      <td>0.320096</td>\n",
       "      <td>0.076971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffc795862ed68dd2c7783dfb8c2d98c</th>\n",
       "      <td>0.094864</td>\n",
       "      <td>0.891130</td>\n",
       "      <td>0.014006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         A         B   NEITHER\n",
       "ID                                                            \n",
       "ffe42e03c8033e9389c762f815075aa3  0.833172  0.163307  0.003521\n",
       "ffea29920b2ac0bd1fa25f70ace4a6db  0.056278  0.940258  0.003464\n",
       "ffec23d7150d0823fff87c3962c1ea52  0.858616  0.104715  0.036669\n",
       "fff8142c3dba443641dc233c049e2bf2  0.602934  0.320096  0.076971\n",
       "fffc795862ed68dd2c7783dfb8c2d98c  0.094864  0.891130  0.014006"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09486409323325361"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.iloc[12358]['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./submissionf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = pd.read_csv(\"./submissionf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000075809a8e6b062f5fb3c191a8ed52</td>\n",
       "      <td>0.784111</td>\n",
       "      <td>0.208917</td>\n",
       "      <td>0.006972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005d0f3b0a6c9ffbd31a48453029911</td>\n",
       "      <td>0.359820</td>\n",
       "      <td>0.585143</td>\n",
       "      <td>0.055037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007775c40bedd4147a0573d66dc28f8</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>0.931273</td>\n",
       "      <td>0.050334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001194e3fe1234d00198ef6bba4cc588</td>\n",
       "      <td>0.703210</td>\n",
       "      <td>0.271778</td>\n",
       "      <td>0.025012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014bb7085278ef3f9b74f14771caca9</td>\n",
       "      <td>0.224165</td>\n",
       "      <td>0.746670</td>\n",
       "      <td>0.029165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID         A         B   NEITHER\n",
       "0  000075809a8e6b062f5fb3c191a8ed52  0.784111  0.208917  0.006972\n",
       "1  0005d0f3b0a6c9ffbd31a48453029911  0.359820  0.585143  0.055037\n",
       "2  0007775c40bedd4147a0573d66dc28f8  0.018394  0.931273  0.050334\n",
       "3  001194e3fe1234d00198ef6bba4cc588  0.703210  0.271778  0.025012\n",
       "4  0014bb7085278ef3f9b74f14771caca9  0.224165  0.746670  0.029165"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
